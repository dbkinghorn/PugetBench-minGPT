{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows how one can generate text given a prompt and some hyperparameters, using either minGPT or huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "from mingpt.model import GPT\n",
    "from mingpt.utils import set_seed\n",
    "from mingpt.bpe import BPETokenizer\n",
    "set_seed(3407)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_mingpt = True # use minGPT or huggingface/transformers model?\n",
    "model_type = 'gpt2-xl'\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 1557.61M\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5074c45210b4f67a460ffc2c07ef45e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30493a9340b140fabfda287437ab637a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if use_mingpt:\n",
    "    model = GPT.from_pretrained(model_type)\n",
    "else:\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "    model.config.pad_token_id = model.config.eos_token_id # suppress a warning\n",
    "\n",
    "# ship model to device and set to eval mode\n",
    "model.to(device)\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(prompt='', num_samples=10, steps=20, do_sample=True):\n",
    "        \n",
    "    # tokenize the input prompt into integer input sequence\n",
    "    if use_mingpt:\n",
    "        tokenizer = BPETokenizer()\n",
    "        if prompt == '':\n",
    "            # to create unconditional samples...\n",
    "            # manually create a tensor with only the special <|endoftext|> token\n",
    "            # similar to what openai's code does here https://github.com/openai/gpt-2/blob/master/src/generate_unconditional_samples.py\n",
    "            x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "        else:\n",
    "            x = tokenizer(prompt).to(device)\n",
    "    else:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_type)\n",
    "        if prompt == '': \n",
    "            # to create unconditional samples...\n",
    "            # huggingface/transformers tokenizer special cases these strings\n",
    "            prompt = '<|endoftext|>'\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "        x = encoded_input['input_ids']\n",
    "    \n",
    "    # we'll process all desired num_samples in a batch, so expand out the batch dim\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    # forward the model `steps` times to get samples, in a batch\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('-'*80)\n",
    "        print(out)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers.\n",
      "\n",
      "It was the first time that the university has invested in computer science and software, said John Vickers, president of the UW System. \"In my view, having this one institution focus\n",
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers (UK), Ltd. ⓘ Bracken Mine S.D. Nock, J.M. Stoddart, R.L. Taylor and L. Riddell (2001) Field\n",
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers;Puget Systems Computers. The system uses a processor, graphics engine, and video memory that are connected to various peripherals. Its design is based on the x86 architecture, although Pug\n",
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers\n",
      "\n",
      "Nokia Siemens Networks, Inc\n",
      "\n",
      "Microsoft (Microsoft Corporation)\n",
      "\n",
      "Red Hat\n",
      "\n",
      "Research In Motion Ltd\n",
      "\n",
      "RSA Group (Security Division, RSA Laboratories)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers Inc.\n",
      "\n",
      "Microsoft Corporation\n",
      "\n",
      "Puget Systems Corporation\n",
      "\n",
      "Puget Systems Computer Systems\n",
      "\n",
      "Siemens AG\n",
      "\n",
      "Warp Micro, Inc.\n",
      "\n",
      "Warp Micro\n",
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers) $1,001 - $5,000\n",
      "\n",
      "9395 Liddell, Chris White House Office Cinta Holdings Corporation N/A None (or less than $1,001)\n",
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers, Inc., Chicago, IL\n",
      "\n",
      "Museum of Fine Arts, Boston, MA\n",
      "\n",
      "Museum of Contemporary Art, Los Angeles, CA\n",
      "\n",
      "University of British Columbia, Vancouver, BC,\n",
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers\n",
      "\n",
      "We are now offering a small but powerful and reliable 3.5\" SATA drive to upgrade the performance of your notebook computer, laptop, or workstation. The PUGet 3.5\n",
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers.\n",
      "\n",
      "And then there's the money. While I can't speak to the costs of running Bitcoin mining at home, I can say that over the years the price of a single block has gone\n",
      "--------------------------------------------------------------------------------\n",
      "Puget Systems Computers\n",
      "\n",
      "(1995–2000)\n",
      "\n",
      "(1995–2000) David G. Nason\n",
      "\n",
      "David G. Nason\n",
      "\n",
      "(1991–1993) Programmer\n",
      "\n",
      "(1991–\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='Puget Systems Computers', num_samples=10, steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "pytorch-dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
